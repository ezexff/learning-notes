# Audio

* Light is photon who can move (physical thing)
* Sound is movement in molecules (attribute of things)
* Frequency = pitch - насколько часто звуковая волна вибрирует, насколько часто волна колеблется, в зависимости от частоты звук выше или ниже
* Amplitude - громкость - пик звуковой волны
* Mixing sound by adding them together
* Clipping - амплитуда выcшей верхней границы

#### Sound mixing
* Standart SamplesPerSecond = 48000
* BytesPerSample = 4
* Bytes for 1 second = 48000*4=192000
* MBytes memory we need for 3 minutes sound = 192000*3*60/1024/1024=32.95
* Возможно 64 мегабайта достаточно для стриминг буфера аудио?
* 2 ways of sound mixing: Modulation and Interpolation
* Volume control = sample * volume
* Pan control = sample * pan
* Pitch control = freq * pitch (сделает звук выше и быстрее или ниже и медленее)

Стандартная задержка звука, до улучшений в windows 10, это 10 мс или же 10 / 1000 * 48000 = 480 семплов

Под Windows имеется 4 основных API для работы со звуком
* MME (MultiMedia Extensions)
* DirectSound
* AudioGraph
* WASAPI (Windows Audio Service)

MME очень старый интерфейс и не особо актуальный из-за большой задержки

DirectSound API, на новых версиях Windows, эмулируется из-за чего возникает большая задержка. Реализация звука через DirectSound API имеет смысл только если необходима поддержка старых версий Windows, например, XP

Приложения, которым требуется низкая задержка, могут использовать новые API аудио (AudioGraph или WASAPI) для запроса размеров буферов, поддерживаемых драйвером, и выбрать тот, который будет использоваться для передачи данных на оборудование и с него

Совет MSDN. Какой из двух API следует использовать?
* Предпочитать AudioGraph везде, где это возможно, для разработки новых приложений
* Используйте WASAPI, только если
  * Вам требуется больше контроля, чем у AudioGraph
  * Требуется меньшая задержка, чем у AudioGraph

Задержка у AudioGraph больше чем у WASAPI. Различия в задержке между WASAPI и AudioGraph обусловлены следующими причинами
* AudioGraph добавляет один буфер задержки на стороне захвата, чтобы синхронизировать отрисовку и запись, которые не предоставляются WASAPI. Это дополнение упрощает код для приложений, написанных с помощью AudioGraph
* Есть еще один буфер задержки на стороне отрисовки AudioGraph, когда система использует буферы более 6 мс
* AudioGraph не имеет возможности отключить захват звуковых эффектов

Не было бы лучше, если бы все приложения использовали новые API для низкой задержки? Не гарантирует ли низкая задержка лучшее взаимодействие с пользователем? Не обязательно. Низкая задержка имеет свои компромиссы
* Низкая задержка означает более высокое энергопотребление. Если система использует буферы размером 10 мс, это означает, что ЦП будет просыпаться каждые 10 мс, заполнять буфер данных и переходить в спящий режим. Однако если система использует буферы размером 1 мс, это означает, что ЦП будет пробуждаться каждые 1 мс. Во втором сценарии это означает, что ЦП будет чаще просыпаться и энергопотребление увеличится. Это приведет к уменьшению времени работы батареи
* Большинство приложений используют звуковые эффекты для обеспечения наилучшего взаимодействия с пользователем. Например, проигрыватели мультимедиа хотят обеспечить высокое качество звука. Приложениям связи требуется минимальное количество эха и шума. Добавление этих типов звуковых эффектов в поток увеличивает задержку. Эти приложения больше заинтересованы в качестве звука, чем в задержке звука

Таким образом, каждый тип приложения имеет разные потребности в отношении задержки звука. Если приложению не требуется низкая задержка, оно не должно использовать новые API для низкой задержки

Способы реализации звука в игровом движке
* В главном цикле
* В отдельном потоке

## DirectSound

Алгоритм использования DirectSound API
* Load the library
* Get a DirectSound object (cooperative)
* Setting up WaveFormat
* "Create" a primary buffer
* "Create" a secondary buffer
* Fill data into 2nd buffer: Lock() 2nd buffer, fill data, Unlock() 2nd buffer
* Play() sound

DirectSound primary and secondary buffers not a buffers, they are handles (only 1 memory buffer)

GetCurrentPosition() --- запрос текущих позиций буфера
* PlayCursor --- смещение позиции воспроизведения буфера
* WriteCursor --- смещение позиции записи (обновления данных) буфера
* Позиция записи обычно опережает позицию воспроизведения на 10-15 мс

Проблема синхронизации звука в игровом движке с использованием DirectSound
* На каждом кадре, например, за 33.3ms (1000ms/30fps) нам необходимо успеть подготовить картинку и звук к следующему кадру
* Эта работа (подготовка картинки и звука), происходит через некоторое время после начала кадра (до звука и рендеринга мы собираем инпуты, обновляем игровой мир и т.д.)
* Следовательно, мы обрабатываем звук в последнюю очередь, в конце текущего кадра и перед началом следующего
* В теории, как только мы отправим звук на оборудование, то он будет воспроизведен пользователю, но...
* К сожалению для нас, при обращении к звуковой карте, существует аппаратная задержка. Например, в DirectSound API, этот процесс может иметь задержку в 30ms, т.е. длиною почти в целый кадр!
* Помимо аппаратной задержки, в DirectSound API имеется задержка воспроизведения, связанная с методом GetCurrentPosition() в 10-15 мс
* В сумме, у нас образуется задержка в 3 кадра!
  * При начале работы с аудиокартой у нас задержка 30ms, т.е. 1 целый кадр
  * Мы запрашиваем местоположение курсора в конце кадра, т.е. в конце 2го кадра
  * Так как позиция записи смещена на 10-15 мс, то воспроизводить звук мы сможем лишь на 3ем кадре
* Если мы хотим побороть задержку, то, в идеале, нам нужно отправлять аудио данные до того как начнётся рендрениг кадра, но это создаст фрейм лаг, т.к. старый звук не будет учитывать новые инпуты и новое состояние мира
* Исходя из этого, в подобной системе нет возможности вычислить звук для кадра и воспроизвести его вместе с изображением. Тогда возможны следующие варианты:
  * Принять тот факт, что звук всегда будет отставать на 2 кадра
  * Иметь "режим с низкой задержкой", который будет работать на машине с отличной звуковой картой и низкой задержкой, в противном случае производить вычисления звука с задержкой
  * Устранить проблемы с задержкой звука, получая данные об инпутах, о состояние мира и т.д. за кадр до его воспроизведения. Однако это приведет к задержке ввода, что, кажется, худший компромисс для экшн-игры

Вычисление оборудования с низкой задержкой задержкой в DirectSound
* Для вычисления низкой задержки используем курсор записи WriteCursor
* Если WriteCursor находится в пределах текущего кадра, то предполагаем что имеем оборудование с низкой задержкой
* Смещаем TargerCursor в начало следующего кадра, и, таким образом, звук всегда будет выходить вместе с изображением
* Если WriteCursor выходит за пределы текущего кадра, то предполагаем, что имеем оборудование с высокой задержкой
  * Не ждём начала следующего кадра и наш TargerCursor будет таким, каким будет WriteCursor после одного кадра
  * Так же мы добавляем небольшое смещение к TargetCursor (несколько ms), чтобы компенсировать изменения задержки между текущим моментом и выводом

Теория вычислений вывода звука
* Мы определяем безопасное число семплов на которое может меняться цикл обновления игры (например, 2ms)
* Когда мы записывает звук, то смотрим на позицию PlayCursor, и, прогнозируя заранее, понимаем где PlayCursor окажется на следующем кадре
* Затем мы проверим находится ли WriteCursor, в сумме c безопасным значением, в границах текущего кадра. Если да, то целевая граница записи это начало следующего кадра. Это даст нам идеальную синхронизацию звука в случае звуковой карты с достаточно низкой задержкой
* Если же WriteCursor за пределами безопасного значения, то мы полагаем, что не добьёмся идеальной синхронизации звука и тогда будем записывать звук с следующего кадра плюс безопасное значение


## WASAPI

WASAPI это преемник DirectSound API

Работает в двух режимах: общий и эксклюзивный

Пример возможной реализации WASAPI в цикле игрового движка
* Init (create audioclient and audiorender)
* GameCycle
  * Processing inputs
  * GameUpdateAndRender()
  * GlobalSoundClient->GetCurrentPadding(&SoundPaddingSize))
  * SamplesToWrite = (int)(SoundOutput.BufferSize - SoundPaddingSize);
  * GetSoundSamples()
  * SoundBuffer.SamplesPerSecond = SoundOutput.SamplesPerSecond;
  * SoundBuffer.SampleCount = SamplesToWrite;
  * SoundBuffer.Samples = Samples;
  * Win32FillSoundBuffer
    * GlobalSoundRenderClient->GetBuffer((UINT32)SamplesToWrite, &SoundBufferData)
    * DestSamples = SourceSamples
    * GlobalSoundRenderClient->ReleaseBuffer((UINT32)SamplesToWrite, 0);

Начиная с Windows 10, WASAPI был расширен для
* Разрешить приложению обнаруживать диапазон размеров буфера (то есть значений периодичности), поддерживаемых звуковым драйвером данного звукового устройства. Это позволяет приложению выбирать размер буфера по умолчанию (10 мс) или небольшой буфер (менее 10 мс) при открытии потока в общем режиме. Если приложение не указывает размер буфера, оно будет использовать размер буфера по умолчанию
* Разрешить приложению обнаруживать текущий формат и периодичность звукового модуля. Это позволяет приложениям привязаться к текущим параметрам звукового модуля
* Разрешить приложению указать, что оно хочет выполнять отрисовку и запись в формате, который он указывает, без какой-либо повторной выборки со стороны обработчика звука

Перечисленные выше функции будут доступны на всех устройствах с Windows. Однако некоторые устройства с достаточным количеством ресурсов и обновленными драйверами обеспечивают лучшее взаимодействие с пользователем, чем другие. Описанные выше функции предоставляются в новом интерфейсе IAudioClient3, который является производным от IAudioClient2

IAudioClient3 определяет следующие 3 метода
* GetCurrentSharedModeEnginePeriod() - Возвращает текущий формат и периодичность обработчика звука
* GetSharedModeEnginePeriod() - Возвращает диапазон периодичности, поддерживаемый подсистемой для указанного формата потока
* InitializeSharedAudioStream() - Инициализирует общий поток с указанной периодичностью

